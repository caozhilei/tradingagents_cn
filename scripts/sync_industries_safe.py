#!/usr/bin/env python3
"""
å®‰å…¨åœ°åŒæ­¥è¡Œä¸šæ•°æ®ï¼ˆå¸¦é‡è¯•å’Œé”™è¯¯å¤„ç†ï¼‰
ä½¿ç”¨ä¹‹å‰æµ‹è¯•æˆåŠŸçš„è¡Œä¸šæ¿å—æ¥å£ï¼Œé€æ­¥åŒæ­¥
"""
import asyncio
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, List
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from motor.motor_asyncio import AsyncIOMotorClient
from pymongo import UpdateOne
from app.core.config import settings
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)


async def sync_industries_with_retry(max_retries: int = 3, delay: float = 2.0):
    """å¸¦é‡è¯•æœºåˆ¶çš„è¡Œä¸šæ•°æ®åŒæ­¥"""
    try:
        import akshare as ak
        
        logger.info("=" * 80)
        logger.info("ğŸš€ å¼€å§‹å®‰å…¨åœ°åŒæ­¥è¡Œä¸šæ•°æ®ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰")
        logger.info("=" * 80)
        
        # è¿æ¥MongoDB
        logger.info("\nğŸ”Œ è¿æ¥MongoDB...")
        client = AsyncIOMotorClient(settings.MONGO_URI)
        db = client[settings.MONGO_DB]
        collection = db["stock_basic_info"]
        
        # 1. è·å–è¡Œä¸šæ¿å—åˆ—è¡¨ï¼ˆå¸¦é‡è¯•ï¼‰
        logger.info("\nğŸ“‹ æ­¥éª¤1: è·å–è¡Œä¸šæ¿å—åˆ—è¡¨ï¼ˆå¸¦é‡è¯•ï¼‰...")
        
        industries_df = None
        for retry in range(max_retries):
            try:
                def fetch_industries():
                    return ak.stock_board_industry_name_em()
                
                industries_df = await asyncio.to_thread(fetch_industries)
                if industries_df is not None and not industries_df.empty:
                    logger.info(f"âœ… æˆåŠŸè·å– {len(industries_df)} ä¸ªè¡Œä¸šæ¿å—ï¼ˆé‡è¯• {retry + 1}/{max_retries}ï¼‰")
                    break
            except Exception as e:
                if retry < max_retries - 1:
                    wait_time = delay * (retry + 1)
                    logger.warning(f"âš ï¸  è·å–è¡Œä¸šæ¿å—å¤±è´¥ï¼ˆé‡è¯• {retry + 1}/{max_retries}ï¼‰ï¼Œç­‰å¾… {wait_time}ç§’åé‡è¯•...")
                    await asyncio.sleep(wait_time)
                else:
                    logger.error(f"âŒ è·å–è¡Œä¸šæ¿å—å¤±è´¥ï¼ˆå·²é‡è¯• {max_retries} æ¬¡ï¼‰: {e}")
                    return
        
        if industries_df is None or industries_df.empty:
            logger.error("âŒ æ— æ³•è·å–è¡Œä¸šæ¿å—åˆ—è¡¨ï¼ŒåŒæ­¥ç»ˆæ­¢")
            return
        
        # 2. é€æ­¥è·å–æ¯ä¸ªè¡Œä¸šçš„è‚¡ç¥¨åˆ—è¡¨
        logger.info(f"\nğŸ“Š æ­¥éª¤2: é€æ­¥è·å–å„è¡Œä¸šçš„è‚¡ç¥¨åˆ—è¡¨...")
        logger.info(f"   å…± {len(industries_df)} ä¸ªè¡Œä¸šéœ€è¦å¤„ç†\n")
        
        industry_stock_map: Dict[str, List[str]] = {}
        success_count = 0
        failed_count = 0
        
        for idx, row in industries_df.iterrows():
            industry_name = str(row.get('æ¿å—åç§°', '')).strip()
            if not industry_name:
                continue
            
            # å°è¯•è·å–è¯¥è¡Œä¸šçš„è‚¡ç¥¨åˆ—è¡¨ï¼ˆå¸¦é‡è¯•ï¼‰
            stocks_df = None
            for retry in range(max_retries):
                try:
                    def fetch_stocks():
                        return ak.stock_board_industry_cons_em(symbol=industry_name)
                    
                    stocks_df = await asyncio.to_thread(fetch_stocks)
                    if stocks_df is not None and not stocks_df.empty:
                        break
                except Exception as e:
                    if retry < max_retries - 1:
                        wait_time = delay * (retry + 1)
                        logger.debug(f"    è¡Œä¸š {industry_name} è·å–å¤±è´¥ï¼ˆé‡è¯• {retry + 1}/{max_retries}ï¼‰ï¼Œç­‰å¾… {wait_time}ç§’...")
                        await asyncio.sleep(wait_time)
                    else:
                        logger.warning(f"  âš ï¸  è¡Œä¸š {industry_name} è·å–å¤±è´¥ï¼ˆå·²é‡è¯• {max_retries} æ¬¡ï¼‰: {e}")
                        failed_count += 1
                        break
            
            if stocks_df is not None and not stocks_df.empty:
                # æå–è‚¡ç¥¨ä»£ç 
                stock_codes = []
                for _, stock_row in stocks_df.iterrows():
                    code = str(stock_row.get('ä»£ç ', '')).strip()
                    if code:
                        code = code.zfill(6)
                        stock_codes.append(code)
                        industry_stock_map[code] = industry_name
                
                success_count += 1
                logger.info(f"  âœ… [{idx+1}/{len(industries_df)}] {industry_name}: {len(stock_codes)} åªè‚¡ç¥¨")
            else:
                failed_count += 1
                logger.warning(f"  âš ï¸  [{idx+1}/{len(industries_df)}] {industry_name}: è·å–å¤±è´¥")
            
            # æ·»åŠ å»¶è¿Ÿï¼Œé¿å…APIé™æµ
            await asyncio.sleep(1.0)  # æ¯ä¸ªè¡Œä¸šä¹‹é—´å»¶è¿Ÿ1ç§’
        
        logger.info(f"\nâœ… æˆåŠŸè·å– {success_count} ä¸ªè¡Œä¸šçš„è‚¡ç¥¨æ•°æ®")
        logger.info(f"âš ï¸  å¤±è´¥ {failed_count} ä¸ªè¡Œä¸š")
        logger.info(f"ğŸ“Š å…±è·å– {len(industry_stock_map)} åªè‚¡ç¥¨çš„è¡Œä¸šä¿¡æ¯")
        
        if not industry_stock_map:
            logger.warning("âš ï¸  æ²¡æœ‰è·å–åˆ°ä»»ä½•è¡Œä¸šæ•°æ®ï¼ŒåŒæ­¥ç»ˆæ­¢")
            return
        
        # 3. æ‰¹é‡æ›´æ–°æ•°æ®åº“ï¼ˆå…ˆæ›´æ–°AKShareæ•°æ®æºï¼‰
        logger.info("\nğŸ’¾ æ­¥éª¤3: æ‰¹é‡æ›´æ–°AKShareæ•°æ®æºçš„è¡Œä¸šæ•°æ®...")
        
        operations = []
        update_count = 0
        batch_size = 100
        
        for code, industry in industry_stock_map.items():
            operations.append(
                UpdateOne(
                    {"code": code, "source": "akshare"},
                    {"$set": {"industry": industry, "updated_at": datetime.utcnow()}},
                    upsert=False
                )
            )
            
            # æ‰¹é‡æ‰§è¡Œ
            if len(operations) >= batch_size:
                try:
                    result = await collection.bulk_write(operations, ordered=False)
                    update_count += result.modified_count
                    logger.info(f"  å·²æ›´æ–° {update_count} åªè‚¡ç¥¨çš„è¡Œä¸šæ•°æ®...")
                    operations = []
                except Exception as e:
                    logger.warning(f"  æ‰¹é‡æ›´æ–°å¤±è´¥: {e}")
                    operations = []
        
        # å¤„ç†å‰©ä½™çš„
        if operations:
            try:
                result = await collection.bulk_write(operations, ordered=False)
                update_count += result.modified_count
            except Exception as e:
                logger.warning(f"  æœ€åä¸€æ‰¹æ›´æ–°å¤±è´¥: {e}")
        
        logger.info(f"âœ… å…±æ›´æ–° {update_count} åªAKShareæ•°æ®æºè‚¡ç¥¨çš„è¡Œä¸šæ•°æ®")
        
        # 4. åŒæ­¥åˆ°TDXæ•°æ®æº
        logger.info("\nğŸ’¾ æ­¥éª¤4: å°†è¡Œä¸šæ•°æ®åŒæ­¥åˆ°TDXæ•°æ®æº...")
        
        # è·å–æ‰€æœ‰æœ‰è¡Œä¸šæ•°æ®çš„è‚¡ç¥¨ä»£ç 
        akshare_stocks_with_industry = await collection.find(
            {"source": "akshare", "industry": {"$ne": None, "$ne": "", "$exists": True}},
            {"code": 1, "industry": 1}
        ).to_list(length=None)
        
        tdx_operations = []
        tdx_update_count = 0
        
        for doc in akshare_stocks_with_industry:
            code = doc.get("code")
            industry = doc.get("industry")
            if code and industry:
                # æ›´æ–°æˆ–åˆ›å»ºTDXæ•°æ®æºè®°å½•
                tdx_operations.append(
                    UpdateOne(
                        {"code": code, "source": "tdx"},
                        {
                            "$set": {
                                "code": code,
                                "symbol": code,
                                "industry": industry,
                                "source": "tdx",
                                "updated_at": datetime.utcnow()
                            }
                        },
                        upsert=True
                    )
                )
                
                # æ‰¹é‡æ‰§è¡Œ
                if len(tdx_operations) >= batch_size:
                    try:
                        result = await collection.bulk_write(tdx_operations, ordered=False)
                        tdx_update_count += result.modified_count + result.upserted_count
                        logger.info(f"  å·²åŒæ­¥ {tdx_update_count} æ¡TDXè®°å½•...")
                        tdx_operations = []
                    except Exception as e:
                        logger.warning(f"  æ‰¹é‡åŒæ­¥TDXå¤±è´¥: {e}")
                        tdx_operations = []
        
        # å¤„ç†å‰©ä½™çš„
        if tdx_operations:
            try:
                result = await collection.bulk_write(tdx_operations, ordered=False)
                tdx_update_count += result.modified_count + result.upserted_count
            except Exception as e:
                logger.warning(f"  æœ€åä¸€æ‰¹TDXåŒæ­¥å¤±è´¥: {e}")
        
        logger.info(f"âœ… å…±åŒæ­¥ {tdx_update_count} æ¡TDXæ•°æ®æºè®°å½•")
        
        # 5. éªŒè¯ç»“æœ
        logger.info("\nğŸ“Š æ­¥éª¤5: éªŒè¯åŒæ­¥ç»“æœ...")
        
        # AKShareæ•°æ®æº
        akshare_with_industry = await collection.count_documents({
            "source": "akshare",
            "industry": {"$ne": None, "$ne": "", "$exists": True}
        })
        akshare_total = await collection.count_documents({"source": "akshare"})
        
        logger.info(f"  AKShareæ•°æ®æº:")
        logger.info(f"    è‚¡ç¥¨æ€»æ•°: {akshare_total}")
        logger.info(f"    æœ‰è¡Œä¸šæ•°æ®: {akshare_with_industry}")
        logger.info(f"    è¦†ç›–ç‡: {akshare_with_industry*100//akshare_total if akshare_total > 0 else 0}%")
        
        # TDXæ•°æ®æº
        tdx_with_industry = await collection.count_documents({
            "source": "tdx",
            "industry": {"$ne": None, "$ne": "", "$exists": True}
        })
        tdx_total = await collection.count_documents({"source": "tdx"})
        
        logger.info(f"  TDXæ•°æ®æº:")
        logger.info(f"    è‚¡ç¥¨æ€»æ•°: {tdx_total}")
        logger.info(f"    æœ‰è¡Œä¸šæ•°æ®: {tdx_with_industry}")
        logger.info(f"    è¦†ç›–ç‡: {tdx_with_industry*100//tdx_total if tdx_total > 0 else 0}%")
        
        # ç»Ÿè®¡è¡Œä¸šåˆ†å¸ƒ
        pipeline = [
            {
                "$match": {
                    "source": {"$in": ["akshare", "tdx"]},
                    "industry": {"$ne": None, "$ne": "", "$exists": True}
                }
            },
            {
                "$group": {
                    "_id": "$industry",
                    "count": {"$sum": 1}
                }
            },
            {"$sort": {"count": -1}},
            {"$limit": 20}
        ]
        
        industries = []
        async for doc in collection.aggregate(pipeline):
            industries.append({
                "industry": doc.get("_id"),
                "count": doc.get("count", 0)
            })
        
        if industries:
            logger.info(f"\nğŸ“Š å‰20ä¸ªè¡Œä¸šåˆ†å¸ƒ:")
            for i, ind in enumerate(industries, 1):
                logger.info(f"  {i}. {ind['industry']}: {ind['count']}åªè‚¡ç¥¨")
        
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ‰ è¡Œä¸šæ•°æ®åŒæ­¥å®Œæˆï¼")
        logger.info("=" * 80)
        
        client.close()
        
    except ImportError:
        logger.error("âŒ akshareåº“æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install akshare")
    except Exception as e:
        logger.error(f"âŒ åŒæ­¥å¤±è´¥: {e}", exc_info=True)


if __name__ == "__main__":
    asyncio.run(sync_industries_with_retry(max_retries=3, delay=2.0))













