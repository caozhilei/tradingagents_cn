#!/usr/bin/env python3
"""
ä»ç”³ä¸‡è¡Œä¸šåˆ†ç±»åŒæ­¥è¡Œä¸šæ•°æ®
ä½¿ç”¨AKShareçš„ç”³ä¸‡è¡Œä¸šåˆ†ç±»æ¥å£è·å–è¡Œä¸šæ•°æ®
"""
import asyncio
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, List

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from motor.motor_asyncio import AsyncIOMotorClient
from pymongo import UpdateOne
from app.core.config import settings
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)


async def sync_industries_from_sw():
    """ä»ç”³ä¸‡è¡Œä¸šåˆ†ç±»åŒæ­¥è¡Œä¸šæ•°æ®"""
    try:
        import akshare as ak
        
        logger.info("=" * 80)
        logger.info("ğŸš€ å¼€å§‹ä»ç”³ä¸‡è¡Œä¸šåˆ†ç±»åŒæ­¥è¡Œä¸šæ•°æ®")
        logger.info("=" * 80)
        
        # è¿æ¥MongoDB
        logger.info("\nğŸ”Œ è¿æ¥MongoDB...")
        client = AsyncIOMotorClient(settings.MONGO_URI)
        db = client[settings.MONGO_DB]
        collection = db["stock_basic_info"]
        
        # 1. è·å–ç”³ä¸‡è¡Œä¸šåˆ†ç±»æ•°æ®
        logger.info("\nğŸ“‹ æ­¥éª¤1: è·å–ç”³ä¸‡è¡Œä¸šåˆ†ç±»æ•°æ®...")
        
        def fetch_sw_industry():
            try:
                # å°è¯•ä½¿ç”¨ç”³ä¸‡è¡Œä¸šåˆ†ç±»æ¥å£
                # æ–¹æ³•1: è·å–ç”³ä¸‡è¡Œä¸šåˆ†ç±»åˆ—è¡¨
                return ak.sw_index_cons(index_code="801010")  # ç”³ä¸‡ä¸€çº§è¡Œä¸šï¼šå†œæ—ç‰§æ¸”
            except:
                try:
                    # æ–¹æ³•2: è·å–ç”³ä¸‡è¡Œä¸šåˆ†ç±»è‚¡ç¥¨åˆ—è¡¨
                    return ak.sw_index_cons(index_code="801010")
                except:
                    # æ–¹æ³•3: è·å–ç”³ä¸‡è¡Œä¸šåˆ†ç±»æŒ‡æ•°åˆ—è¡¨
                    return ak.sw_index_cons(index_code="801010")
        
        # å…ˆå°è¯•è·å–ç”³ä¸‡è¡Œä¸šåˆ†ç±»æŒ‡æ•°åˆ—è¡¨
        logger.info("  å°è¯•è·å–ç”³ä¸‡è¡Œä¸šåˆ†ç±»æŒ‡æ•°åˆ—è¡¨...")
        
        def fetch_sw_index_list():
            try:
                # è·å–ç”³ä¸‡ä¸€çº§è¡Œä¸šæŒ‡æ•°åˆ—è¡¨
                return ak.sw_index_cons(index_code="801010")
            except Exception as e:
                logger.warning(f"  è·å–ç”³ä¸‡æŒ‡æ•°åˆ—è¡¨å¤±è´¥: {e}")
                return None
        
        # å°è¯•ä¸åŒçš„ç”³ä¸‡è¡Œä¸šåˆ†ç±»æ¥å£
        sw_data = None
        industry_stock_map: Dict[str, str] = {}  # {code: industry}
        
        # æ–¹æ³•1: å°è¯•è·å–ç”³ä¸‡è¡Œä¸šåˆ†ç±»è‚¡ç¥¨åˆ—è¡¨ï¼ˆé€šè¿‡è¡Œä¸šä»£ç ï¼‰
        logger.info("  æ–¹æ³•1: é€šè¿‡ç”³ä¸‡è¡Œä¸šä»£ç è·å–è‚¡ç¥¨åˆ—è¡¨...")
        
        # ç”³ä¸‡ä¸€çº§è¡Œä¸šä»£ç åˆ—è¡¨ï¼ˆå¸¸è§è¡Œä¸šï¼‰
        sw_industry_codes = [
            "801010",  # å†œæ—ç‰§æ¸”
            "801020",  # é‡‡æ˜
            "801030",  # åŒ–å·¥
            "801040",  # é’¢é“
            "801050",  # æœ‰è‰²é‡‘å±
            "801080",  # ç”µå­
            "801110",  # å®¶ç”¨ç”µå™¨
            "801120",  # é£Ÿå“é¥®æ–™
            "801130",  # çººç»‡æœè£…
            "801140",  # è½»å·¥åˆ¶é€ 
            "801150",  # åŒ»è¯ç”Ÿç‰©
            "801160",  # å…¬ç”¨äº‹ä¸š
            "801170",  # äº¤é€šè¿è¾“
            "801180",  # æˆ¿åœ°äº§
            "801200",  # å•†ä¸šè´¸æ˜“
            "801210",  # ä¼‘é—²æœåŠ¡
            "801230",  # ç»¼åˆ
            "801710",  # å»ºç­‘ææ–™
            "801720",  # å»ºç­‘è£…é¥°
            "801730",  # ç”µæ°”è®¾å¤‡
            "801740",  # å›½é˜²å†›å·¥
            "801750",  # è®¡ç®—æœº
            "801760",  # ä¼ åª’
            "801770",  # é€šä¿¡
            "801780",  # é“¶è¡Œ
            "801790",  # éé“¶é‡‘è
            "801880",  # æ±½è½¦
            "801890",  # æœºæ¢°è®¾å¤‡
        ]
        
        # ç”³ä¸‡è¡Œä¸šä»£ç åˆ°è¡Œä¸šåç§°çš„æ˜ å°„
        sw_industry_names = {
            "801010": "å†œæ—ç‰§æ¸”",
            "801020": "é‡‡æ˜",
            "801030": "åŒ–å·¥",
            "801040": "é’¢é“",
            "801050": "æœ‰è‰²é‡‘å±",
            "801080": "ç”µå­",
            "801110": "å®¶ç”¨ç”µå™¨",
            "801120": "é£Ÿå“é¥®æ–™",
            "801130": "çººç»‡æœè£…",
            "801140": "è½»å·¥åˆ¶é€ ",
            "801150": "åŒ»è¯ç”Ÿç‰©",
            "801160": "å…¬ç”¨äº‹ä¸š",
            "801170": "äº¤é€šè¿è¾“",
            "801180": "æˆ¿åœ°äº§",
            "801200": "å•†ä¸šè´¸æ˜“",
            "801210": "ä¼‘é—²æœåŠ¡",
            "801230": "ç»¼åˆ",
            "801710": "å»ºç­‘ææ–™",
            "801720": "å»ºç­‘è£…é¥°",
            "801730": "ç”µæ°”è®¾å¤‡",
            "801740": "å›½é˜²å†›å·¥",
            "801750": "è®¡ç®—æœº",
            "801760": "ä¼ åª’",
            "801770": "é€šä¿¡",
            "801780": "é“¶è¡Œ",
            "801790": "éé“¶é‡‘è",
            "801880": "æ±½è½¦",
            "801890": "æœºæ¢°è®¾å¤‡",
        }
        
        success_count = 0
        failed_count = 0
        
        for idx, industry_code in enumerate(sw_industry_codes, 1):
            industry_name = sw_industry_names.get(industry_code, f"è¡Œä¸š{industry_code}")
            
            try:
                def fetch_stocks():
                    try:
                        return ak.sw_index_cons(index_code=industry_code)
                    except:
                        return None
                
                stocks_df = await asyncio.to_thread(fetch_stocks)
                
                if stocks_df is not None and not stocks_df.empty:
                    # æå–è‚¡ç¥¨ä»£ç 
                    stock_count = 0
                    for _, stock_row in stocks_df.iterrows():
                        code = str(stock_row.get('å“ç§ä»£ç ', '') or stock_row.get('ä»£ç ', '') or stock_row.get('code', '')).strip()
                        if code:
                            code = code.zfill(6)
                            industry_stock_map[code] = industry_name
                            stock_count += 1
                    
                    success_count += 1
                    logger.info(f"  âœ… [{idx}/{len(sw_industry_codes)}] {industry_name}: {stock_count} åªè‚¡ç¥¨")
                else:
                    failed_count += 1
                    logger.debug(f"  âš ï¸  [{idx}/{len(sw_industry_codes)}] {industry_name}: æ— æ•°æ®")
                
                # æ·»åŠ å»¶è¿Ÿï¼Œé¿å…APIé™æµ
                await asyncio.sleep(0.5)
                
            except Exception as e:
                failed_count += 1
                logger.warning(f"  âš ï¸  [{idx}/{len(sw_industry_codes)}] {industry_name}: è·å–å¤±è´¥ - {e}")
                await asyncio.sleep(0.5)
                continue
        
        logger.info(f"\nâœ… æˆåŠŸè·å– {success_count} ä¸ªè¡Œä¸šçš„è‚¡ç¥¨æ•°æ®")
        logger.info(f"âš ï¸  å¤±è´¥ {failed_count} ä¸ªè¡Œä¸š")
        logger.info(f"ğŸ“Š å…±è·å– {len(industry_stock_map)} åªè‚¡ç¥¨çš„è¡Œä¸šä¿¡æ¯")
        
        if not industry_stock_map:
            logger.warning("âš ï¸  æ²¡æœ‰è·å–åˆ°ä»»ä½•è¡Œä¸šæ•°æ®ï¼Œå°è¯•å¤‡ç”¨æ–¹æ³•...")
            
            # å¤‡ç”¨æ–¹æ³•ï¼šå°è¯•ä½¿ç”¨å…¶ä»–AKShareæ¥å£
            try:
                logger.info("  å°è¯•ä½¿ç”¨è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯æ¥å£...")
                def fetch_stock_list():
                    return ak.stock_info_a_code_name()
                
                stock_list = await asyncio.to_thread(fetch_stock_list)
                if stock_list is not None and not stock_list.empty:
                    logger.info(f"  æˆåŠŸè·å–è‚¡ç¥¨åˆ—è¡¨ï¼Œå…± {len(stock_list)} åªè‚¡ç¥¨")
                    logger.info("  æ³¨æ„ï¼šæ­¤æ¥å£ä¸åŒ…å«è¡Œä¸šä¿¡æ¯ï¼Œéœ€è¦ä½¿ç”¨å…¶ä»–æ–¹æ³•è·å–è¡Œä¸šæ•°æ®")
            except Exception as e:
                logger.warning(f"  å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥: {e}")
            
            return
        
        # 2. æ‰¹é‡æ›´æ–°AKShareæ•°æ®æº
        logger.info("\nğŸ’¾ æ­¥éª¤2: æ‰¹é‡æ›´æ–°AKShareæ•°æ®æºçš„è¡Œä¸šæ•°æ®...")
        
        operations = []
        update_count = 0
        batch_size = 100
        
        for code, industry in industry_stock_map.items():
            operations.append(
                UpdateOne(
                    {"code": code, "source": "akshare"},
                    {"$set": {"industry": industry, "updated_at": datetime.utcnow()}},
                    upsert=False
                )
            )
            
            if len(operations) >= batch_size:
                try:
                    result = await collection.bulk_write(operations, ordered=False)
                    update_count += result.modified_count
                    logger.info(f"  å·²æ›´æ–° {update_count} åªè‚¡ç¥¨çš„è¡Œä¸šæ•°æ®...")
                    operations = []
                except Exception as e:
                    logger.warning(f"  æ‰¹é‡æ›´æ–°å¤±è´¥: {e}")
                    operations = []
        
        if operations:
            try:
                result = await collection.bulk_write(operations, ordered=False)
                update_count += result.modified_count
            except Exception as e:
                logger.warning(f"  æœ€åä¸€æ‰¹æ›´æ–°å¤±è´¥: {e}")
        
        logger.info(f"âœ… å…±æ›´æ–° {update_count} åªAKShareæ•°æ®æºè‚¡ç¥¨çš„è¡Œä¸šæ•°æ®")
        
        # 3. åŒæ­¥åˆ°TDXæ•°æ®æº
        logger.info("\nğŸ’¾ æ­¥éª¤3: å°†è¡Œä¸šæ•°æ®åŒæ­¥åˆ°TDXæ•°æ®æº...")
        
        tdx_operations = []
        tdx_update_count = 0
        
        for code, industry in industry_stock_map.items():
            # è·å–è‚¡ç¥¨åç§°
            stock_doc = await collection.find_one(
                {"code": code, "source": "akshare"},
                {"name": 1}
            )
            stock_name = stock_doc.get("name", f"è‚¡ç¥¨{code}") if stock_doc else f"è‚¡ç¥¨{code}"
            
            tdx_operations.append(
                UpdateOne(
                    {"code": code, "source": "tdx"},
                    {
                        "$set": {
                            "code": code,
                            "symbol": code,
                            "name": stock_name,
                            "industry": industry,
                            "source": "tdx",
                            "updated_at": datetime.utcnow()
                        }
                    },
                    upsert=True
                )
            )
            
            if len(tdx_operations) >= batch_size:
                try:
                    result = await collection.bulk_write(tdx_operations, ordered=False)
                    tdx_update_count += result.modified_count + result.upserted_count
                    logger.info(f"  å·²åŒæ­¥ {tdx_update_count} æ¡TDXè®°å½•...")
                    tdx_operations = []
                except Exception as e:
                    logger.warning(f"  æ‰¹é‡åŒæ­¥TDXå¤±è´¥: {e}")
                    tdx_operations = []
        
        if tdx_operations:
            try:
                result = await collection.bulk_write(tdx_operations, ordered=False)
                tdx_update_count += result.modified_count + result.upserted_count
            except Exception as e:
                logger.warning(f"  æœ€åä¸€æ‰¹TDXåŒæ­¥å¤±è´¥: {e}")
        
        logger.info(f"âœ… å…±åŒæ­¥ {tdx_update_count} æ¡TDXæ•°æ®æºè®°å½•")
        
        # 4. éªŒè¯ç»“æœ
        logger.info("\nğŸ“Š æ­¥éª¤4: éªŒè¯åŒæ­¥ç»“æœ...")
        
        for source in ['akshare', 'tdx']:
            total = await collection.count_documents({"source": source})
            with_industry = await collection.count_documents({
                "source": source,
                "industry": {"$ne": None, "$ne": "", "$exists": True}
            })
            logger.info(f"  {source.upper()}æ•°æ®æº:")
            logger.info(f"    è‚¡ç¥¨æ€»æ•°: {total}")
            logger.info(f"    æœ‰è¡Œä¸šæ•°æ®: {with_industry}")
            logger.info(f"    è¦†ç›–ç‡: {with_industry*100//total if total > 0 else 0}%")
        
        # ç»Ÿè®¡è¡Œä¸šåˆ†å¸ƒ
        pipeline = [
            {
                "$match": {
                    "source": {"$in": ["akshare", "tdx"]},
                    "industry": {"$ne": None, "$ne": "", "$exists": True}
                }
            },
            {
                "$group": {
                    "_id": "$industry",
                    "count": {"$sum": 1}
                }
            },
            {"$sort": {"count": -1}},
            {"$limit": 30}
        ]
        
        industries = []
        async for doc in collection.aggregate(pipeline):
            industries.append({
                "industry": doc.get("_id"),
                "count": doc.get("count", 0)
            })
        
        if industries:
            logger.info(f"\nğŸ“Š å‰30ä¸ªè¡Œä¸šåˆ†å¸ƒ:")
            for i, ind in enumerate(industries, 1):
                logger.info(f"  {i}. {ind['industry']}: {ind['count']}åªè‚¡ç¥¨")
        
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ‰ ç”³ä¸‡è¡Œä¸šåˆ†ç±»æ•°æ®åŒæ­¥å®Œæˆï¼")
        logger.info("=" * 80)
        
        client.close()
        
    except ImportError:
        logger.error("âŒ akshareåº“æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install akshare")
    except Exception as e:
        logger.error(f"âŒ åŒæ­¥å¤±è´¥: {e}", exc_info=True)


if __name__ == "__main__":
    asyncio.run(sync_industries_from_sw())


