
import logging
from fastapi import APIRouter, HTTPException, Depends
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
from app.routers.auth_db import get_current_user

from app.services.screening_service import ScreeningService, ScreeningParams
from app.services.enhanced_screening_service import get_enhanced_screening_service
from app.services.mcp_screening_service import MCPScreeningService
from app.models.screening import (
    ScreeningCondition, ScreeningRequest as NewScreeningRequest,
    ScreeningResponse as NewScreeningResponse, FieldInfo, BASIC_FIELDS_INFO
)
from app.core.response import ok

router = APIRouter(tags=["screening"])
logger = logging.getLogger("webapi")

# ç­›é€‰å­—æ®µé…ç½®å“åº”æ¨¡å‹
class FieldConfigResponse(BaseModel):
    """ç­›é€‰å­—æ®µé…ç½®å“åº”"""
    fields: Dict[str, FieldInfo]
    categories: Dict[str, List[str]]

# ä¼ ç»Ÿçš„è¯·æ±‚/å“åº”æ¨¡å‹ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
class OrderByItem(BaseModel):
    field: str
    direction: str = Field("desc", pattern=r"^(?i)(asc|desc)$")

class ScreeningRequest(BaseModel):
    market: str = Field("CN", description="å¸‚åœºï¼šCN")
    date: Optional[str] = Field(None, description="äº¤æ˜“æ—¥YYYY-MM-DDï¼Œç¼ºçœä¸ºæœ€æ–°")
    adj: str = Field("qfq", description="å¤æƒå£å¾„ï¼šqfq/hfq/noneï¼ˆP0å ä½ï¼‰")
    conditions: Dict[str, Any] = Field(default_factory=dict)
    order_by: Optional[List[OrderByItem]] = None
    limit: int = Field(50, ge=1, le=500)
    offset: int = Field(0, ge=0)

class ScreeningResponse(BaseModel):
    total: int
    items: List[dict]

# æœåŠ¡å®ä¾‹
svc = ScreeningService()
enhanced_svc = get_enhanced_screening_service()
mcp_screening_svc = MCPScreeningService()


@router.get("/fields", response_model=FieldConfigResponse)
async def get_screening_fields(user: dict = Depends(get_current_user)):
    """
    è·å–ç­›é€‰å­—æ®µé…ç½®
    è¿”å›æ‰€æœ‰å¯ç”¨çš„ç­›é€‰å­—æ®µåŠå…¶é…ç½®ä¿¡æ¯
    """
    try:
        # å­—æ®µåˆ†ç±»
        categories = {
            "basic": ["code", "name", "industry", "area", "market"],
            "market_value": ["total_mv", "circ_mv"],
            "financial": ["pe", "pb", "pe_ttm", "pb_mrq", "roe"],
            "trading": ["turnover_rate", "volume_ratio"],
            "price": ["close", "pct_chg", "amount"],
            "technical": ["ma20", "rsi14", "kdj_k", "kdj_d", "kdj_j", "dif", "dea", "macd_hist"]
        }

        return FieldConfigResponse(
            fields=BASIC_FIELDS_INFO,
            categories=categories
        )

    except Exception as e:
        logger.error(f"[get_screening_fields] è·å–å­—æ®µé…ç½®å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


def _convert_legacy_conditions_to_new_format(legacy_conditions: Dict[str, Any]) -> List[ScreeningCondition]:
    """
    å°†ä¼ ç»Ÿæ ¼å¼çš„ç­›é€‰æ¡ä»¶è½¬æ¢ä¸ºæ–°æ ¼å¼

    ä¼ ç»Ÿæ ¼å¼ç¤ºä¾‹:
    {
        "logic": "AND",
        "children": [
            {"field": "market_cap", "op": "between", "value": [5000000, 9007199254740991]}
        ]
    }

    æ–°æ ¼å¼:
    [
        ScreeningCondition(field="total_mv", operator="between", value=[50, 90071992547])
    ]
    """
    conditions = []

    # å­—æ®µåæ˜ å°„ï¼ˆå‰ç«¯å¯èƒ½ä½¿ç”¨çš„æ—§å­—æ®µå -> ç»Ÿä¸€çš„åç«¯å­—æ®µåï¼‰
    field_mapping = {
        "market_cap": "total_mv",      # å¸‚å€¼ï¼ˆå…¼å®¹æ—§å­—æ®µåï¼‰
        "pe_ratio": "pe",              # å¸‚ç›ˆç‡ï¼ˆå…¼å®¹æ—§å­—æ®µåï¼‰
        "pb_ratio": "pb",              # å¸‚å‡€ç‡ï¼ˆå…¼å®¹æ—§å­—æ®µåï¼‰
        "turnover": "turnover_rate",   # æ¢æ‰‹ç‡ï¼ˆå…¼å®¹æ—§å­—æ®µåï¼‰
        "change_percent": "pct_chg",   # æ¶¨è·Œå¹…ï¼ˆå…¼å®¹æ—§å­—æ®µåï¼‰
        "price": "close",              # ä»·æ ¼ï¼ˆå…¼å®¹æ—§å­—æ®µåï¼‰
    }

    # æ“ä½œç¬¦æ˜ å°„
    operator_mapping = {
        "between": "between",
        "gt": ">",
        "lt": "<",
        "gte": ">=",
        "lte": "<=",
        "eq": "==",
        "ne": "!=",
        "in": "in",
        "contains": "contains"
    }

    if isinstance(legacy_conditions, dict):
        children = legacy_conditions.get("children", [])

        for child in children:
            if isinstance(child, dict):
                field = child.get("field")
                op = child.get("op")
                value = child.get("value")

                if field and op and value is not None:
                    # æ˜ å°„å­—æ®µå
                    mapped_field = field_mapping.get(field, field)

                    # æ˜ å°„æ“ä½œç¬¦
                    mapped_op = operator_mapping.get(op, op)

                    # å¤„ç†å¸‚å€¼å•ä½è½¬æ¢ï¼ˆå‰ç«¯ä¼ å…¥çš„æ˜¯ä¸‡å…ƒï¼Œæ•°æ®åº“å­˜å‚¨çš„æ˜¯äº¿å…ƒï¼‰
                    if mapped_field == "total_mv" and isinstance(value, list):
                        # å°†ä¸‡å…ƒè½¬æ¢ä¸ºäº¿å…ƒ
                        converted_value = [v / 10000 for v in value if isinstance(v, (int, float))]
                        logger.info(f"[screening] å¸‚å€¼å•ä½è½¬æ¢: {value} ä¸‡å…ƒ -> {converted_value} äº¿å…ƒ")
                        value = converted_value
                    elif mapped_field == "total_mv" and isinstance(value, (int, float)):
                        value = value / 10000
                        logger.info(f"[screening] å¸‚å€¼å•ä½è½¬æ¢: {child.get('value')} ä¸‡å…ƒ -> {value} äº¿å…ƒ")

                    # åˆ›å»ºç­›é€‰æ¡ä»¶
                    condition = ScreeningCondition(
                        field=mapped_field,
                        operator=mapped_op,
                        value=value
                    )
                    conditions.append(condition)

                    logger.info(f"[screening] è½¬æ¢æ¡ä»¶: {field}({op}) -> {mapped_field}({mapped_op}), å€¼: {value}")

    return conditions


# ä¼ ç»Ÿç­›é€‰æ¥å£ï¼ˆä¿æŒå‘åå…¼å®¹ï¼Œä½†ä½¿ç”¨å¢å¼ºæœåŠ¡ï¼‰
@router.post("/run", response_model=ScreeningResponse)
async def run_screening(req: ScreeningRequest, user: dict = Depends(get_current_user)):
    try:
        logger.info(f"[screening] è¯·æ±‚æ¡ä»¶: {req.conditions}")
        logger.info(f"[screening] æ’åºä¸åˆ†é¡µ: order_by={req.order_by}, limit={req.limit}, offset={req.offset}")

        # è½¬æ¢ä¼ ç»Ÿæ ¼å¼çš„æ¡ä»¶ä¸ºæ–°æ ¼å¼
        conditions = _convert_legacy_conditions_to_new_format(req.conditions)
        logger.info(f"[screening] è½¬æ¢åçš„æ¡ä»¶: {conditions}")

        # ä½¿ç”¨å¢å¼ºç­›é€‰æœåŠ¡
        result = await enhanced_svc.screen_stocks(
            conditions=conditions,
            market=req.market,
            date=req.date,
            adj=req.adj,
            limit=req.limit,
            offset=req.offset,
            order_by=[{"field": o.field, "direction": o.direction} for o in (req.order_by or [])],
            use_database_optimization=True
        )

        logger.info(f"[screening] ç­›é€‰å®Œæˆ: total={result.get('total')}, "
                   f"took={result.get('took_ms')}ms, optimization={result.get('optimization_used')}")

        if result.get('items'):
            sample = result['items'][:3]
            logger.info(f"[screening] è¿”å›æ ·ä¾‹(å‰3æ¡): {sample}")

        return ScreeningResponse(total=result["total"], items=result["items"])

    except Exception as e:
        logger.error(f"[screening] å¤„ç†å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


# æ–°çš„ä¼˜åŒ–ç­›é€‰æ¥å£
@router.post("/enhanced", response_model=NewScreeningResponse)
async def enhanced_screening(req: NewScreeningRequest, user: dict = Depends(get_current_user)):
    """
    å¢å¼ºçš„è‚¡ç¥¨ç­›é€‰æ¥å£
    - æ”¯æŒæ›´ä¸°å¯Œçš„ç­›é€‰æ¡ä»¶æ ¼å¼
    - è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜çš„ç­›é€‰ç­–ç•¥ï¼ˆæ•°æ®åº“ä¼˜åŒ– vs ä¼ ç»Ÿæ–¹æ³•ï¼‰
    - æä¾›è¯¦ç»†çš„æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯
    """
    try:
        logger.info(f"[enhanced_screening] ç­›é€‰æ¡ä»¶: {len(req.conditions)}ä¸ª")
        logger.info(f"[enhanced_screening] æ’åºä¸åˆ†é¡µ: order_by={req.order_by}, limit={req.limit}, offset={req.offset}")

        # æ‰§è¡Œå¢å¼ºç­›é€‰
        result = await enhanced_svc.screen_stocks(
            conditions=req.conditions,
            market=req.market,
            date=req.date,
            adj=req.adj,
            limit=req.limit,
            offset=req.offset,
            order_by=req.order_by,
            use_database_optimization=req.use_database_optimization
        )

        logger.info(f"[enhanced_screening] ç­›é€‰å®Œæˆ: total={result.get('total')}, "
                   f"took={result.get('took_ms')}ms, optimization={result.get('optimization_used')}")

        return NewScreeningResponse(
            total=result["total"],
            items=result["items"],
            took_ms=result.get("took_ms"),
            optimization_used=result.get("optimization_used"),
            source=result.get("source")
        )

    except Exception as e:
        logger.error(f"[enhanced_screening] ç­›é€‰å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å¢å¼ºç­›é€‰å¤±è´¥: {str(e)}")


# è·å–æ”¯æŒçš„å­—æ®µä¿¡æ¯
@router.get("/fields", response_model=List[Dict[str, Any]])
async def get_supported_fields(user: dict = Depends(get_current_user)):
    """è·å–æ‰€æœ‰æ”¯æŒçš„ç­›é€‰å­—æ®µä¿¡æ¯"""
    try:
        fields = await enhanced_svc.get_all_supported_fields()
        return fields
    except Exception as e:
        logger.error(f"[screening] è·å–å­—æ®µä¿¡æ¯å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–å­—æ®µä¿¡æ¯å¤±è´¥: {str(e)}")


# è·å–å•ä¸ªå­—æ®µçš„è¯¦ç»†ä¿¡æ¯
@router.get("/fields/{field_name}", response_model=Dict[str, Any])
async def get_field_info(field_name: str, user: dict = Depends(get_current_user)):
    """è·å–æŒ‡å®šå­—æ®µçš„è¯¦ç»†ä¿¡æ¯"""
    try:
        field_info = await enhanced_svc.get_field_info(field_name)
        if not field_info:
            raise HTTPException(status_code=404, detail=f"å­—æ®µ '{field_name}' ä¸å­˜åœ¨")
        return field_info
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[screening] è·å–å­—æ®µä¿¡æ¯å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–å­—æ®µä¿¡æ¯å¤±è´¥: {str(e)}")


# éªŒè¯ç­›é€‰æ¡ä»¶
@router.post("/validate", response_model=Dict[str, Any])
async def validate_conditions(conditions: List[ScreeningCondition], user: dict = Depends(get_current_user)):
    """éªŒè¯ç­›é€‰æ¡ä»¶çš„æœ‰æ•ˆæ€§"""
    try:
        validation_result = await enhanced_svc.validate_conditions(conditions)
        return validation_result
    except Exception as e:
        logger.error(f"[screening] éªŒè¯æ¡ä»¶å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"éªŒè¯æ¡ä»¶å¤±è´¥: {str(e)}")

# é‡å¤å®šä¹‰çš„æ—§ç«¯ç‚¹ç§»é™¤ï¼ˆä¿ç•™å¸¦æ—¥å¿—çš„ç‰ˆæœ¬ï¼‰


@router.get("/industries")
async def get_industries(user: dict = Depends(get_current_user)):
    """
    è·å–æ•°æ®åº“ä¸­æ‰€æœ‰å¯ç”¨çš„è¡Œä¸šåˆ—è¡¨
    æ ¹æ®ç³»ç»Ÿé…ç½®çš„æ•°æ®æºä¼˜å…ˆçº§ï¼Œä»æ‰€æœ‰å¯ç”¨çš„æ•°æ®æºè·å–è¡Œä¸šåˆ†ç±»æ•°æ®ï¼ˆåŒ…æ‹¬TDXï¼‰
    å¦‚æœæ•°æ®åº“ä¸­çš„è¡Œä¸šæ•°æ®ä¸è¶³ï¼Œä»AKShareå®æ—¶è·å–è¡Œä¸šåˆ—è¡¨ä½œä¸ºè¡¥å……
    è¿”å›æŒ‰è‚¡ç¥¨æ•°é‡æ’åºçš„è¡Œä¸šåˆ—è¡¨
    """
    try:
        from app.core.database import get_mongo_db
        from app.core.unified_config import UnifiedConfigManager
        import asyncio

        db = get_mongo_db()
        collection = db["stock_basic_info"]

        # ğŸ”¥ è·å–æ•°æ®æºä¼˜å…ˆçº§é…ç½®ï¼ˆä½¿ç”¨ç»Ÿä¸€é…ç½®ç®¡ç†å™¨çš„å¼‚æ­¥æ–¹æ³•ï¼‰
        config = UnifiedConfigManager()
        data_source_configs = await config.get_data_source_configs_async()

        # æå–å¯ç”¨çš„æ•°æ®æºï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åºï¼ˆåŒ…æ‹¬TDXï¼‰
        enabled_sources = []
        for ds in data_source_configs:
            if not ds.enabled:
                continue
            # å¤„ç†æšä¸¾ç±»å‹ï¼šå¦‚æœtypeæ˜¯æšä¸¾ï¼Œè·å–valueï¼›å¦åˆ™ç›´æ¥ä½¿ç”¨
            ds_type = ds.type.value if hasattr(ds.type, 'value') else str(ds.type)
            ds_type_lower = ds_type.lower()
            if ds_type_lower in ['tushare', 'akshare', 'baostock', 'tdx']:
                enabled_sources.append(ds_type_lower)

        if not enabled_sources:
            # å¦‚æœæ²¡æœ‰é…ç½®ï¼Œä½¿ç”¨é»˜è®¤é¡ºåº
            enabled_sources = ['tushare', 'akshare', 'baostock', 'tdx']

        logger.info(f"[get_industries] æ•°æ®æºä¼˜å…ˆçº§: {enabled_sources}")

        # ğŸ”¥ ä»æ‰€æœ‰å¯ç”¨çš„æ•°æ®æºæŸ¥è¯¢è¡Œä¸šï¼ˆåˆå¹¶ç»“æœï¼‰
        # é¦–å…ˆå°è¯•ä»æ•°æ®åº“æŸ¥è¯¢æ‰€æœ‰å¯ç”¨æ•°æ®æºçš„è¡Œä¸šæ•°æ®
        pipeline = [
            {
                "$match": {
                    "source": {"$in": enabled_sources},  # ğŸ”¥ æŸ¥è¯¢æ‰€æœ‰å¯ç”¨çš„æ•°æ®æºï¼ˆåŒ…æ‹¬TDXï¼‰
                    "industry": {"$ne": None, "$ne": "", "$exists": True}  # è¿‡æ»¤ç©ºè¡Œä¸š
                }
            },
            {
                "$group": {
                    "_id": "$industry",
                    "count": {"$sum": 1},
                    "sources": {"$addToSet": "$source"}  # è®°å½•è¯¥è¡Œä¸šæ¥è‡ªå“ªäº›æ•°æ®æº
                }
            },
            {"$sort": {"count": -1}},  # æŒ‰è‚¡ç¥¨æ•°é‡é™åºæ’åº
            {
                "$project": {
                    "industry": "$_id",
                    "count": 1,
                    "sources": 1,
                    "_id": 0
                }
            }
        ]

        industries = []
        industry_dict = {}  # ç”¨äºå»é‡å’Œåˆå¹¶
        async for doc in collection.aggregate(pipeline):
            # æ¸…æ´—å­—æ®µï¼Œé¿å… NaN/Inf å¯¼è‡´ JSON åºåˆ—åŒ–å¤±è´¥
            raw_industry = doc.get("industry")
            safe_industry = ""
            try:
                if raw_industry is None:
                    safe_industry = ""
                elif isinstance(raw_industry, float):
                    if raw_industry != raw_industry or raw_industry in (float("inf"), float("-inf")):
                        safe_industry = ""
                    else:
                        safe_industry = str(raw_industry)
                else:
                    safe_industry = str(raw_industry).strip()
            except Exception:
                safe_industry = ""

            if not safe_industry:  # è·³è¿‡ç©ºè¡Œä¸š
                continue

            raw_count = doc.get("count", 0)
            safe_count = 0
            try:
                if isinstance(raw_count, float):
                    if raw_count != raw_count or raw_count in (float("inf"), float("-inf")):
                        safe_count = 0
                    else:
                        safe_count = int(raw_count)
                else:
                    safe_count = int(raw_count)
            except Exception:
                safe_count = 0

            # å¦‚æœè¡Œä¸šå·²å­˜åœ¨ï¼Œåˆå¹¶è®¡æ•°ï¼ˆå–æœ€å¤§å€¼ï¼‰
            if safe_industry in industry_dict:
                industry_dict[safe_industry]["count"] = max(industry_dict[safe_industry]["count"], safe_count)
            else:
                industry_dict[safe_industry] = {
                    "value": safe_industry,
                    "label": safe_industry,
                    "count": safe_count,
                }

        industries = list(industry_dict.values())
        industries.sort(key=lambda x: x["count"], reverse=True)  # æŒ‰è‚¡ç¥¨æ•°é‡é™åºæ’åº

        logger.info(f"[get_industries] ä»æ•°æ®åº“è¿”å› {len(industries)} ä¸ªè¡Œä¸šï¼ˆæ•°æ®æº: {enabled_sources}ï¼‰")

        # ğŸ”¥ å¦‚æœè¡Œä¸šæ•°æ®ä¸è¶³ï¼ˆå°‘äº10ä¸ªï¼‰ï¼Œå°è¯•ä»AKShareå®æ—¶è·å–è¡¥å……ï¼ˆé™åˆ¶æ•°é‡é¿å…è¶…æ—¶ï¼‰
        if len(industries) < 10:
            logger.info(f"[get_industries] æ•°æ®åº“è¡Œä¸šæ•°æ®ä¸è¶³ï¼ˆ{len(industries)}ä¸ªï¼‰ï¼Œå°è¯•ä»AKShareå®æ—¶è·å–è¡¥å……ï¼ˆé‡‡æ ·50åªè‚¡ç¥¨ï¼‰...")
            try:
                from app.services.data_sources.akshare_adapter import AKShareAdapter
                import akshare as ak

                akshare_adapter = AKShareAdapter()
                if akshare_adapter.is_available():
                    # ä»AKShareè·å–è‚¡ç¥¨åˆ—è¡¨å¹¶æå–è¡Œä¸šä¿¡æ¯ï¼ˆé‡‡æ ·æ–¹å¼ï¼‰
                    def fetch_stock_list():
                        try:
                            # ä½¿ç”¨AKShareè·å–Aè‚¡è‚¡ç¥¨åˆ—è¡¨
                            return ak.stock_info_a_code_name()
                        except Exception as e:
                            logger.warning(f"[get_industries] AKShareè·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
                            return None

                    stock_list_df = await asyncio.to_thread(fetch_stock_list)
                    if stock_list_df is not None and not stock_list_df.empty:
                        # æå–è‚¡ç¥¨ä»£ç ï¼ˆ6ä½ï¼‰ï¼Œé‡‡æ ·50åªè‚¡ç¥¨ï¼ˆæ¯20åªå–1åªï¼Œè¦†ç›–ä¸åŒå¸‚åœºï¼‰
                        stock_codes_all = stock_list_df['code'].apply(lambda x: str(x).zfill(6)).tolist()
                        # é‡‡æ ·ç­–ç•¥ï¼šå–å‰50åªï¼Œè¦†ç›–ä¸åŒä»£ç æ®µ
                        sample_size = min(50, len(stock_codes_all))
                        step = max(1, len(stock_codes_all) // sample_size)
                        stock_codes = stock_codes_all[::step][:sample_size]
                        
                        # æ‰¹é‡è·å–è¡Œä¸šä¿¡æ¯
                        akshare_industries = {}
                        success_count = 0
                        for code in stock_codes:
                            try:
                                def fetch_stock_info():
                                    try:
                                        return ak.stock_individual_info_em(symbol=code)
                                    except:
                                        return None
                                
                                stock_info = await asyncio.to_thread(fetch_stock_info)
                                if stock_info is not None and not stock_info.empty:
                                    # æå–è¡Œä¸šä¿¡æ¯
                                    industry_row = stock_info[stock_info['item'] == 'æ‰€å±è¡Œä¸š']
                                    if not industry_row.empty:
                                        industry_name = str(industry_row['value'].iloc[0]).strip()
                                        if industry_name and industry_name not in ['-', '--', 'æœªçŸ¥', '']:
                                            if industry_name not in akshare_industries:
                                                akshare_industries[industry_name] = 0
                                            akshare_industries[industry_name] += 1
                                            success_count += 1
                                
                                # æ·»åŠ å»¶è¿Ÿï¼Œé¿å…APIé™æµ
                                if success_count % 10 == 0:  # æ¯10ä¸ªè¯·æ±‚åå»¶è¿Ÿç¨é•¿
                                    await asyncio.sleep(0.2)
                                else:
                                    await asyncio.sleep(0.05)
                            except Exception as e:
                                logger.debug(f"[get_industries] è·å–{code}è¡Œä¸šä¿¡æ¯å¤±è´¥: {e}")
                                continue

                        # åˆå¹¶AKShareçš„è¡Œä¸šæ•°æ®
                        if akshare_industries:
                            for industry_name, count in akshare_industries.items():
                                if industry_name in industry_dict:
                                    # å¦‚æœå·²å­˜åœ¨ï¼Œæ›´æ–°è®¡æ•°ï¼ˆå–è¾ƒå¤§å€¼ï¼‰
                                    industry_dict[industry_name]["count"] = max(industry_dict[industry_name]["count"], count)
                                else:
                                    # æ–°å¢è¡Œä¸š
                                    industry_dict[industry_name] = {
                                        "value": industry_name,
                                        "label": industry_name,
                                        "count": count,
                                    }

                            industries = list(industry_dict.values())
                            industries.sort(key=lambda x: x["count"], reverse=True)
                            logger.info(f"[get_industries] ä»AKShareè¡¥å……äº† {len(akshare_industries)} ä¸ªè¡Œä¸šï¼Œå…± {len(industries)} ä¸ªè¡Œä¸š")
            except Exception as e:
                logger.warning(f"[get_industries] ä»AKShareè·å–è¡Œä¸šæ•°æ®å¤±è´¥: {e}")

        # ç¡®å®šå®é™…ä½¿ç”¨çš„æ•°æ®æº
        used_sources = []
        if industries:
            used_sources = list(set(enabled_sources))
        if len(industries) > 0 and any('akshare' in str(ind).lower() for ind in industries[:10]):
            if 'akshare' not in used_sources:
                used_sources.append('akshare')

        return {
            "industries": industries,
            "total": len(industries),
            "source": "+".join(used_sources) if used_sources else "database"  # ğŸ”¥ è¿”å›æ•°æ®æ¥æº
        }

    except Exception as e:
        logger.error(f"[get_industries] è·å–è¡Œä¸šåˆ—è¡¨å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


# MCPæ™ºèƒ½é€‰è‚¡æŸ¥è¯¢
class MCPQueryRequest(BaseModel):
    """MCPæŸ¥è¯¢è¯·æ±‚"""
    query: str = Field(..., description="ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼Œä¾‹å¦‚ï¼š'PE<20ä¸”ROE>15%çš„è‚¡ç¥¨'ã€'æ¶¨åœçš„è‚¡ç¥¨'ç­‰")
    market: str = Field("AG", description="å¸‚åœºç±»å‹ï¼šAG=Aè‚¡ï¼ŒJJ=åŸºé‡‘ï¼ŒZS=æŒ‡æ•°")
    max_results: int = Field(50, ge=1, le=200, description="æœ€å¤§è¿”å›ç»“æœæ•°")


class MCPQueryResponse(BaseModel):
    """MCPæŸ¥è¯¢å“åº”"""
    success: bool
    message: str
    stocks: List[Dict[str, Any]]
    query: str
    original_query: str
    total: int
    columns: List[str]


@router.post("/mcp-query")
async def mcp_query_stocks(
    request: MCPQueryRequest,
    user: dict = Depends(get_current_user)
):
    """
    ä½¿ç”¨MCPå·¥å…·è¿›è¡Œæ™ºèƒ½é€‰è‚¡æŸ¥è¯¢
    
    æ”¯æŒçš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢ç¤ºä¾‹ï¼š
    - "PE<20ä¸”ROE>15%çš„è‚¡ç¥¨"
    - "æ¶¨åœçš„è‚¡ç¥¨"
    - "æ¶¨å¹…è¶…è¿‡5%çš„è‚¡ç¥¨"
    - "MACDé‡‘å‰çš„è‚¡ç¥¨"
    - "ä¸»åŠ›èµ„é‡‘å‡€æµå…¥çš„è‚¡ç¥¨"
    - "åŠå¯¼ä½“è¡Œä¸šPEä¸­ä½æ•°"
    - "çªç ´20æ—¥å‡çº¿çš„è‚¡ç¥¨"
    """
    try:
        result = await mcp_screening_svc.query_stocks_with_llm(
            user_query=request.query,
            market=request.market,
            max_results=request.max_results
        )
        # éªŒè¯ç»“æœæ ¼å¼ï¼ˆç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½å­˜åœ¨ï¼‰
        try:
            response_data = MCPQueryResponse(**result)
            return ok(data=response_data.dict())
        except Exception as validation_error:
            logger.error(f"[MCPé€‰è‚¡] å“åº”éªŒè¯å¤±è´¥: {validation_error}")
            # å¦‚æœéªŒè¯å¤±è´¥ï¼Œç›´æ¥è¿”å›ç»“æœï¼ˆç¡®ä¿åŒ…å«æ‰€æœ‰å¿…éœ€å­—æ®µï¼‰
            return ok(data=result)
    except Exception as e:
        logger.error(f"[MCPé€‰è‚¡] æŸ¥è¯¢å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"MCPæŸ¥è¯¢å¤±è´¥: {str(e)}")